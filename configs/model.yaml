# Model configurations for ForgetGate-V
# Supports ViT and CNN architectures

# Vision Transformer configurations (using timm)
vit:
  # ViT-Tiny (fast for testing)
  tiny:
    model_name: vit_tiny_patch16_224
    pretrained: true
    input_size: 224
    patch_size: 16
    embed_dim: 192
    num_heads: 3
    num_layers: 12
    mlp_ratio: 4.0
    drop_rate: 0.0
    attn_drop_rate: 0.0

  # ViT-Small
  small:
    model_name: vit_small_patch16_224
    pretrained: true
    input_size: 224
    patch_size: 16
    embed_dim: 384
    num_heads: 6
    num_layers: 12
    mlp_ratio: 4.0
    drop_rate: 0.0
    attn_drop_rate: 0.0

  # ViT-Base
  base:
    model_name: vit_base_patch16_224
    pretrained: true
    input_size: 224
    patch_size: 16
    embed_dim: 768
    num_heads: 12
    num_layers: 12
    mlp_ratio: 4.0
    drop_rate: 0.0
    attn_drop_rate: 0.0

# CNN configurations (using torchvision)
cnn:
  # ResNet-18
  resnet18:
    model_name: resnet18
    pretrained: true
    input_size: 224
    num_classes: null  # Will be set based on dataset

  # ResNet-34
  resnet34:
    model_name: resnet34
    pretrained: true
    input_size: 224
    num_classes: null

  # ResNet-50
  resnet50:
    model_name: resnet50
    pretrained: true
    input_size: 224
    num_classes: null

# LoRA configuration for PEFT
lora:
  # Target modules for LoRA adaptation
  vit_target_modules: ["attn.qkv", "attn.proj", "mlp.fc1", "mlp.fc2"]
  # For ResNet, target individual conv layers within BasicBlocks
  # Pattern: layer{N}.{M}.conv{1,2} targets conv1 and conv2 in each BasicBlock
  cnn_target_modules:
    - "layer1.0.conv1"
    - "layer1.0.conv2"
    - "layer1.1.conv1"
    - "layer1.1.conv2"
    - "layer2.0.conv1"
    - "layer2.0.conv2"
    - "layer2.1.conv1"
    - "layer2.1.conv2"
    - "layer3.0.conv1"
    - "layer3.0.conv2"
    - "layer3.1.conv1"
    - "layer3.1.conv2"
    - "layer4.0.conv1"
    - "layer4.0.conv2"
    - "layer4.1.conv1"
    - "layer4.1.conv2"
    - "fc"  # Final fully connected layer

  # LoRA hyperparameters
  r: 8  # Low-rank dimension
  lora_alpha: 16
  lora_dropout: 0.05
  bias: "none"

# VPT (Visual Prompt Tuning) configuration
vpt:
  # Prompt types
  prompt_type: "prefix"  # "prefix" or "patch"
  prompt_length: 5  # Number of prompt tokens
  prompt_dim: 192  # Should match ViT-Tiny embed_dim (192 for tiny, 384 for small, 768 for base)
  patch_size: 16  # For patch-based prompts
  prompt_dropout: 0.0

# Training settings
training:
  optimizer: adamw
  lr: 1e-3
  weight_decay: 0.01
  scheduler: cosine
  warmup_steps: 100
  max_epochs: 100
  early_stopping_patience: 10

# Checkpoint settings
checkpoint:
  save_every: 10
  keep_last: 3
  monitor: "val_acc"
  mode: "max"
